
@InProceedings{2018_7,
author    = {Sarwate, Avneesh and Tsuchiya, Takahiko and Freeman, Jason},
title     = {Collaborative Coding with Music: Two Case Studies with EarSketch},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {This paper describes the motivation, design, and implementation of new features in EarSketch that enable the collaborative creation of algorithmic music. EarSketch is a web-based Digital Audio Workstation (DAW), designed primarily for educational contexts, in which users author Python or JavaScript code to programmatically create music within a multi-track paradigm. In this paper, we describe these new collaborative features in EarSketch and discuss their potential for use in both educational and music performance contexts.},
file      = {:Users/Hjem/Documents/WAC Papers/2018/collaborative-coding-with-music-two-case-studies-with-earsketch.pdf:pdf},
type      = {Paper},
}

@InProceedings{2018_10,
author    = {Major, Oliver},
title     = {DSP2JS A C ++ framework for the development of in-browser DSPs},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {We present DSP2JS, a new framework for the develop- ment of audio signal processors for web platforms using Em- scripten and the WebAudio API. In particular, the goal is to abstract common functionality in a configurable layer that manages the communication between a JavaScript applica- tion and DSP code written in C or C++. The framework includes functionality for the creation, connection and man- agement of processing units, runtime profiling, buffer man- agement, buffer conversion and a configurable build system. The proposed three-step development of a signal proces- sor with DSP2JS allows for external libraries to be included, making it possible to port existing code to the framework. The generated artifacts can then be used in a web page and invoked via an interface similar to native WebAudio- Nodes. The optional omission of WebAudio bindings via a bare-build mode potentially opens up the core framework to further DSP applications, even outside of the audio domain. We examine the multilayered architecture of the core framework and the build system, also discussing design and implemetation decisions.},
file      = {:Users/Hjem/Documents/WAC Papers/2018/dsp2js-a-cplusplus-framework-for-the-development-of-in-browser-dsps.pdf:pdf},
type      = {Paper},
}

@InProceedings{2018_5,
author    = {Baumann, Christian and Friederike, Johanna and Milde, Jan-Torsten},
title     = {Body Movement Sonification using the Web Audio API},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
pages     = {5--8},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {In this paper we describe the ongoing research on the devel- opment of a body movement sonification system. High precision, high resolution wireless sensors are used to track the body movement and record muscle excitation. We are currently using 6 sensors. In the final version of the system full body tracking can be achieved. The recording system provides a web server including a simple REST API, which streams the recorded data in JSON format. An inter- mediate proxy server pre-processes the data and transmits it to the final sonification system. The sonification system is implemented using the web au- dio api. We are experimenting with a set of different soni- fication strategies and algorithms. Currently we are testing the system as part of an interactive, guided therapy, estab- lishing additional acoustic feedback channels for the patient. In a second stage of the research we are going to use the sys- tem in a more musical and artistic way. More specifically we plan to use the system in cooperation with a violist, where the acoustic feedback channel will be integrated into the performance.},
file      = {:Users/Hjem/Documents/WAC Papers/2018/body-movement-sonification-using-the-web-audio-api.pdf:pdf},
type      = {Paper},
}

@InProceedings{2018_8,
author    = {Fyfe, Lawrence and Gladin, Olivier and Fleury, Cédric and Beaudouin-Lafon, Michel},
title     = {Combining Web Audio Streaming, Motion Capture, and Binaural Audio in a Telepresence System},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {This paper describes the use of spatialized binaural 3D audio in the Digiscape telepresence system. Digiscape is a custom-built system that enables groups of users of visualization platforms, including CAVEs and wall-sized displays, to collaborate at a distance while visualizing and manipulating large and complex data sets. Digiscape supports web-based audio and video streaming as well as data sharing among its constituent platforms. Using motion capture systems, the locations of collaborators in each physical spaces are sent to the remote platforms, which spatialize the collaborators' audio streams in their local audio space. The audio spaces can be configured in a variety of ways, from a single shared audio space to mapped, adjacent, contained, split, or distorted audio spaces, facilitating the exploration of the possibilities of spatialized voice communication in telepresence systems.},
file      = {:Users/Hjem/Documents/WAC Papers/2018/combining-web-audio-streaming-motion-capture-and-binaural-audio-in-a-telepresence-system.pdf:pdf},
keywords  = {Opus,RTP,Telepresence,WebRTC,binaural audio,motion capture,remote collaboration},
type      = {Paper},
url       = {https://hal.inria.fr/hal-01957843/},
}

@InProceedings{2018_9,
author    = {Matuszewski, Benjamin and Larralde, Joseph and Bevilacqua, Frédéric},
title     = {Designing Movement Driven Audio Applications Using a Web-Based Interactive Machine Learning Toolkit},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
pages     = {2--5},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {This paper presents a web based toolkit for implementing Interactive Machine Learning (IML) dedicated to creative audio applications. The toolkit, composed of a main library and a template application , facilitates the creation of experiences on collective musical interactions with a strong emphasis on real-time movement processing and recognition. At its lower level, the mano-js library proposes a user-friendly API built on top of existing libraries. The library is designed to assist developers and creative coders in the appropriation and usage of the Interactive Machine Learning concepts and workflow, as well as to simplify development of new applications. The library is open-source, based on web standards and released under the BSD-3-Clause Licence. At its higher level, the toolkit proposes Elements, a template application designed towards non-developer users. The application specifically aims at providing a mean for researchers and designers to prototype new movement-based distributed Interactive Machine Learning scenarios. The application allows to create a new scenario by simply providing a JSON configuration file that defines the role and the abilities of each client. The application has been iteratively tested and developed in the context of several workshops.},
file      = {:Users/Hjem/Documents/WAC Papers/2018/designing-movement-driven-audio-applications-using-a-web-based-interactive-machine-learning-toolkit.pdf:pdf},
type      = {Paper},
url       = {http://reactable.com/snap/},
}

@InProceedings{2018_2,
author    = {Xambó, Anna and Tremblay, Pierre Alexandre and Roma, Gerard and Green, Owen},
title     = {A Javascript Library for Flexible Visualization of Audio Descriptors},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
pages     = {3--8},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {Research in audio analysis has provided a large number of ways to describe audio recordings, which can be used for enhancing their visual representation in web applications. In this paper we present fav.js, a Javascript library for flexible visualization of audio descriptors. We explain the proposed design and demonstrate its potential for web audio applications through several visualization examples.},
file      = {:Users/Hjem/Documents/WAC Papers/2018/a-javascript-library-for-flexible-visualization-of-audio-descriptors.pdf:pdf},
type      = {Paper},
url       = {https://www.vamp-plugins.org},
}

@InProceedings{2018_3,
author    = {Carson, Tate},
title     = {A more perfect union: Composition with audience-controlled smartphone speaker array and evolutionary computer music},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {A more perfect union incorporates an audience-controlled smartphone speaker array with evolutionary computer mu- sic. A genetic algorithm drives the work and the perfor- mance practice that the audience follows.},
file      = {:Users/Hjem/Documents/WAC Papers/2018/a-more-perfect-union-composition-with-audience-controlled-smartphone-speaker-array-and-evolutionary-computer-music.pdf:pdf},
type      = {Paper},
}

@InProceedings{2018_6,
author    = {Houge, Ben},
title     = {Cena concertante alla maniera di Vivaldi: Considering the Restaurant as a Musical Interface},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {In recent years, I have been conducting research into the links between music and gastronomy by collaborating with chefs to develop multisensory dining experiences that I call “food operas.” These events incorporate real-time music techniques adapted from the world of video game development to respond to the unpredictable events and timings of the dining room. This paper details an event I developed in collaboration with the Boston Symphony Orchestra, chef David Verdo, and designer Jutta Friedrichs that took place on January 5 and 7, 2017. Cena concertante alla maniera di Vivaldi was a four-course meal with real- time musical accompaniment deployed from a seventy-channel speaker array comprised of sixty-four iPads and six near field studio monitors, all coordinated via the same network. The iPads were positioned in custom-built acoustic resonators placed at each seat in the restaurant, presenting a unique audio channel to each diner, synchronized to the rhythms of each diner's meal and sited as close as possible to the food. The music was based on Vivaldi's Piccolo Concerto in C Major, RV 443, drawing from archival BSO performances, with the objective of enhancing diners' appreciation of a live performance of the work on a concert following the meal. The menu was based on the music, drawing on research in the field of crossmodal psychology that identifies links between the senses of taste and hearing. This paper discusses the background of the project, its musical organization, the infrastructure and control techniques required to execute it, and relevant research in the field of crossmodal psychology, concluding with a discussion of areas for future work.},
file      = {:Users/Hjem/Documents/WAC Papers/2018/cena-concertante-alla-maniera-di-vivaldi-considering-the-testaurant-as-a-musical-interface.pdf:pdf},
type      = {Paper},
}

@InProceedings{2018_1,
author    = {Hespanhol, Nuno and Rodrigues, Óscar and Gomes, José Alberto},
title     = {0+1=SOM: Bringing Computing Closer to Children Through Music},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
pages     = {0--3},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {0 + 1 = SOM is a project developed by Digitópia - Casa da Música in cooperation with Braga Media Arts. In this project we developed a series of online tools that use technology and basic mathematical and logical concepts, such as counting and understanding a loop or an if condition, to create music. These tools were later used in a series of four workshops with elementary school children as a creative activity that complements the classroom. This paper describes the process and reasoning behind the creation of the tools (explaining our choices when creating the contents), the role Web Audio played in it (particularly in the ability to schedule precise audio events), the results we have achieved so far, and the feedback we have had from students and teachers. We also discuss further applications and plans for the future.},
file      = {:Users/Hjem/Documents/WAC Papers/2018/0plus1equalssom-bringing-computing-closer-to-children-through-music.pdf:pdf},
keywords  = {composition,elementary education,intuitive manner,making music in an,mathematics,music,music education,software applications,that allow creating and,the increasing number of,this is related to},
type      = {Paper},
}

@InProceedings{2018_4,
author    = {Thalmann, Florian and Thompson, Lucas and Sandler, Mark},
title     = {A User-Adaptive Automated DJ Web App with Object-Based Audio and Crowd-Sourced Decision Trees},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
volume    = {c},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {We describe the concepts behind a web-based minimal-UI DJ system that adapts to the user's preference via sim- ple interactive decisions and feedback on taste. Starting from a preset decision tree modeled on common DJ prac- tice, the system can gradually learn a more customised and user-specific tree. At the core of the system are structural representations of the musical content based on semantic au- dio technologies and inferred from features extracted from the audio directly in the browser. These representations are gradually combined into a representation of the mix which could then be saved and shared with other users. We show how different types of transitions can be modeled using sim- ple musical constraints. Potential applications of the system include crowd-sourced data collection, both on temporally aligned playlisting and musical preference.},
file      = {:Users/Hjem/Documents/WAC Papers/2018/a-user-adaptive-automated-dj-web-app-with-object-based-audio-and-crowd-sourced-decision-trees.pdf:pdf},
type      = {Paper},
}

@InProceedings{2018_24,
author    = {Yi, Steven and Lazzarini, Victor and Costello, Ed},
title     = {WebAssembly AudioWorklet Csound},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {This paper describes WebAssembly AudioWorklet (WAAW) Csound, one of the implementations of Web Audio Csound. We begin by introducing the background to this current im- plementation, stemming from the two first ports of Csound to the web platform using Native Clients and asm.js. The technology of WebAssembly is then introduced and dis- cussed in its more relevant aspects. The AudioWorklet inter- face of Web Audio API is explored, together with its use in WAAW Csound. We complement this discussion by consid- ering the overarching question of support for multiple plat- forms, which implement different versions of Web Audio. Some initial examples of the system are presented to illus- trate various potential applications. Finally, we complement the paper by discussing current issues that are fundamental for this project and others that rely on the development of a robust support for WASM-based audio computing.},
file      = {:Users/Hjem/Documents/WAC Papers/2018/webassembly-audioworklet-csound.pdf:pdf},
type      = {Paper},
}

@InProceedings{2018_23,
author    = {Cretti, Francesco and Morino, Luca and Liuni, Marco and Gervasoni, Stefano and Agostini, Andrea and Servetti, Antonio},
title     = {Web Wall Whispers : an interactive web-based sound work},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
pages     = {2--3},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {Web Wall Whispers (www) is an interactive sound work that heavily relies on the web audio technology to enable a vir- tual high-quality multimodal exploration of a monumental mural. The user's navigation through the artwork generates a unique interactive musical composition at every access, in a challenging paradigm of open form based on a virtual dia- logue between the visitors and the composer. The project is conceived as a part of the Segni per la Speranza (spls, Signs for Hope) multimodal artwork, a project aimed at the reap- praisal of urban outlying areas. All the constituent materials are freely distributed under the open source GNU General Public Licence, thus allowing the build-up of extensions or new versions of this multimodal artwork paradigm.},
file      = {:Users/Hjem/Documents/WAC Papers/2018/web-wall-whispers.pdf:pdf},
type      = {Paper},
}

@InProceedings{2018_21,
author    = {Marasco, Anthony T and Allison, Jesse},
title     = {SoundSling : A Framework for Using Creative Motion Data to Pan Audio Across a Mobile Device Speaker Array},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
pages     = {2--4},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {SoundSling is a framework used to translate motion-based data into audio diffusion trajectories across a crowd of net- worked mobile devices. The intent is to allow a performer to distribute audio across audience mobile devices in cre- ative ways with motion data that mimics various patterns of movement found in the natural world. As a sound is “slung” around the room, the software intuitively adjusts each audience member's gain as it moves past their loca- tion. SoundSling adapts dynamically to the total number of devices as users connect to or disconnect from the network. This helps to ensure that the performer's chosen diffusion patterns and motion trajectories can be scaled properly to the array of currently-participating devices. Existing as a collection of MaxMSP abstractions and easily-editable web page templates, a focus has been kept on making the tool as adaptable to a performer's current musical set-up as possi- ble.},
file      = {:Users/Hjem/Documents/WAC Papers/2018/soundsling-a-framework-for-using-creative-motion-data-to-pan-audio-across-a-mobile-device-speaker-array.pdf:pdf},
type      = {Paper},
}

@InProceedings{2018_25,
author    = {Buffa, Michel and Lebrun, Jerome},
title     = {WebAudio Virtual Tube Guitar Amps and Pedal Board Design},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {In this paper are exposed our latest experiments with the WebAudio API to design different types of gears for guitarists: real-time simulations of tube guitars amplifiers, fx pedals, and their integration in a virtual pedal board. We have studied different real guitar tube amps and created an interactive Web application for experimenting, validating and building different amp designs that can be run in browsers. Blind tests have been conducted with professional guitar players who compared positively our real-time, low-latency, realistic tube guitar amps simulations with state-of-the-art native equivalents. We also created a set of “virtual audio fx pedals” that implement popular audio effects such as flanger, chorus, overdrive, pitch shifter etc. These amps and pedals simulations can be packaged as “WebAudio plugins” and stored in plugin repositories (REST endpoints or local folders). We also developed a “host” application -a virtual pedal board- that allows us to scan repositories for plugins and to chain/assemble them.},
file      = {:Users/Hjem/Documents/WAC Papers/2018/webaudio-virtual-tube-guitar-amps-and-pedal-board-design.pdf:pdf},
type      = {Paper},
}

@InProceedings{2018_22,
author    = {Buffa, Michel and Lebrun, Jerome and Kleimola, Jari and Larkin, Oliver and Pellerin, Guillaume and Letz, Stéphane},
title     = {WAP: Ideas for a Web Audio Plug-in Standard},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {Several native audio plug-in formats are popular today including Steinberg's VST, Apple's Audio Units, Avid's AAX and the Linux audio community's LV2. Although the APIs are different, all exist to achieve more or less the same thing - represent an instrument or audio effect and allow it to be loaded by a host application. In the Web Audio API such a high-level audio plug-in entity does not exist. With the emergence of web-based audio software such as digital audio workstations (DAWs), it is desirable to have a standard in order to make Web Audio instruments and effects interoperable. Since there are many ways of developing for Web Audio, such a standard should be flexible enough to support different approaches, including using a variety of programming languages. New functionality that is enabled by the web platform should be available to plug-ins written in different ways. To this end, several groups of developers came together to make their work compatible, and this paper presents the work achieved so far. This includes the development of a draft API specification, a small preliminary SDK, online plug-in validators and a set of examples written in JavaScript. These simple, proof of concept examples show how to discover plug-ins from repositories, how to instantiate a plug-in and how to connect plug-ins together. A more ambitious host has also been developed to validate the WAP standard: a virtual guitar “pedal board” that discovers plug-ins from multiple remote repositories, and allows the musician to chain pedals and control them via MIDI.},
file      = {:Users/Hjem/Documents/WAC Papers/2018/wap-ideas-for-a-web-audio-plug-in-standard.pdf:pdf},
type      = {Paper},
}

@InProceedings{2018_20,
author    = {Fiala, Jakub},
title     = {r-audio: Declarative, reactive and flexible Web Audio graphs in React Dependency on React},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {Web Audio is by design an object-oriented, imperative API offering low-level control over audio graphs. There have been a number of efforts to provide a more intuitive wrapper API. Designing such wrapper libraries poses challenges in address- ing graph configuration, dynamic mutation and data flow. Syntax of creating directed graphs in imperative code is not representative of the complex graph shapes, making the code difficult to understand without external visualisation tools. In this paper I describe r-audio1, a Web Audio wrapper li- brary which attempts to solve the issues of imperative graph representations by leveraging the component system of Re- act. I compare approaches of existing wrapper libraries and discuss solutions to specific issues of declarative and reactive representations of Web Audio graphs. I evaluate r-audio in terms of the ability to create arbitrary directed graphs and mutate them in real time.},
file      = {:Users/Hjem/Documents/WAC Papers/2018/r-audio-declarative-reactive-and-flexible-web-audio-graphs-in-react.pdf:pdf},
keywords  = {declarative,directed graphs,javascript,react,web audio},
type      = {Paper},
}

@InProceedings{2018_19,
author    = {Pauwels, Johan and Sandler, Mark B},
title     = {pywebaudioplayer : Bridging the gap between audio processing code in Python and attractive visualisations based on web technology},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {Lately, a number of audio players based on web technol- ogy have made it possible for researchers to present their audio-related work in an attractive manner. Tools such as wavesurfer.js, waveform-playlist and trackswitch.js provide highly-configurable players, allowing a more interactive ex- ploration of scientific results that goes beyond simple linear playback. However, the audio output to be presented is in many cases not generated by the same web technologies. The pro- cess of preparing audio data for display therefore requires manual intervention, in order to bridge the resulting gap between programming languages. While this is acceptable for one-time events, such as the preparation of final results, it prevents the usage of such players during the iterative de- velopment cycle. Having access to rich audio players already during development would allow researchers to get more in- stantaneous feedback. The current workflow consists of re- peatedly importing audio into a digital audio workstation in order to achieve similar capabilities, a repetitive and time- consuming process. In order to address these needs, we present pywebaudio- player, a Python package that automates the generation of code snippets for the each of the three aforementioned web audio players. It is aimed at use-cases where audio develop- ment in Python is combined with web visualisation. Notable examples are Jupyter Notebook and WSGI-compatible web frameworks such as Flask or Django.},
file      = {:Users/Hjem/Documents/WAC Papers/2018/pywebaudioplayer-bridging-the-gap-between-audio-processing-code-in-python-and-attractive-visualisations-based-on-web-technology.pdf:pdf},
keywords  = {audio player,multi-track audio,python},
type      = {Paper},
}

@InProceedings{2018_18,
author    = {Stolfi, Ariane and Milo, Alessia and Ceriani, Miguel and Barthet, Mathieu},
title     = {Participatory musical improvisations with Playsound.space},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {Playsound.space is a web-based tool to search for and play Creative Commons licensed-sounds, which can be applied to free improvisation, experimental music production and soundscape composition. It provides fast access to about 400k non-musical and musical sounds provided by Freesound and allows users to play/loop single or multiple sounds retrieved through text-based search. Sound discovery is facilitated by the use of semantic searches and sound visual representations (spectrograms). After feedback gathered from user tests and practices with the tool as an instrument, we identified several directions to develop the expressive and collaborative capabilities of the tool. We present additional features for more complex audio processing, and also to enhance participation trough a chat system that allows users to share sound sessions and exchange messages while playing.},
file      = {:Users/Hjem/Documents/WAC Papers/2018/participatory-musical-improvisations-with-playsound-space.pdf:pdf},
type      = {Paper},
url       = {https://youtu.be/yv8T70rawzs.},
}

@InProceedings{2018_17,
author    = {Kleimola, Jari and Campbell, Owen},
title     = {Native Web Audio API Plugins},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
pages     = {4--9},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {This work enables native audio plugin development using the Web Audio API and other web technologies. Hybrid forms where DSP algorithms are implemented in both JavaScript and native C++, and distributed forms where web technologies are used only for the user interface, are also supported. Various implementation options are explored, and the most promising option is implemented and evaluated. We found that the solution is able to operate at 128 sample buffer sizes, and that the performance of the Web Audio API audio graph is not compromised. The proof-of- concept solution also maintains compatibility with existing Web Audio API implementations. The average MIDI latency was 24 ms, which is high when comparing with fully native plugin solutions. Backwards compatibility also reduces usability when working with multiple plugin instances. We conclude that the second iteration needs to break backwards compatibility in order to overcome the MIDI latency and multi-plugin support issues.},
file      = {:Users/Hjem/Documents/WAC Papers/2018/native-web-audio-api-plugins.pdf:pdf},
type      = {Paper},
}

@InProceedings{2018_16,
author        = {Favory, Xavier and Serra, Xavier},
title         = {Multi Web Audio Sequencer : Collaborative Music Making},
booktitle     = {Proceedings of the International Web Audio Conference},
year          = {2018},
editor        = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series        = {WAC '18},
address       = {Berlin},
month         = sep,
publisher     = {TU Berlin},
abstract      = {Recent advancements in web-based audio systems have enabled sufficiently accurate timing control and real-time sound processing capabilities. Numerous specialized music tools, as well as digital audio workstations, are now accessi- ble from browsers. Features such as the large accessibility of data and real-time communication between clients make the web attractive for collaborative data manipulation. How- ever, this innovative field has yet to produce effective tools for multiple-user coordination on specialized music creation tasks. The Multi Web Audio Sequencer is a prototype of an application for segment-based sequencing of Freesound sound clips, with an emphasis on seamless remote collabo- ration. In this work we consider a fixed-grid step sequencer as a probe for understanding the necessary features of crowd- shared music creation sessions. This manuscript describes the sequencer and the functionalities and types of interac- tions required for effective and attractive collaboration of remote people during creative music creation activities.},
archiveprefix = {arXiv},
arxivid       = {arXiv:1905.06717v1},
eprint        = {arXiv:1905.06717v1},
file          = {:Users/Hjem/Documents/WAC Papers/2018/multi-web-audio-sequencer-collaborative-music-making.pdf:pdf},
type          = {Paper},
}

@InProceedings{2018_15,
author    = {Roberts, Charles},
title     = {Metaprogramming Strategies for AudioWorklets},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {The introduction of AudioWorklets to the Web Audio API greatly expands the potential of browser-based audio pro- gramming. However, managing state between the various threads AudioWorklets occupy entails a fair amount of com- plexity, particularly when designing dynamic music pro- gramming environments where exact digital signal process- ing requirements cannot be known ahead of time. Such en- vironments are commonly used for live coding performance, interactive composition, and coding playgrounds for musical experimentation. Our research explores metaprogramming strategies to cre- ate AudioWorklet implementations for two JavaScript li- braries, Genish.js and Gibberish.js. These strategies help hide the complexities of inter-thread communication from end-users and enable a variety of signal processing and in- teraction techniques that would otherwise be difficult to achieve.},
file      = {:Users/Hjem/Documents/WAC Papers/2018/metaprogramming-strategies-for-audioworklets.pdf:pdf},
type      = {Paper},
}

@InProceedings{2018_14,
author    = {Mitchusson, Chase and Allison, Jesse},
title     = {Lost In Space: Indoor Localization for Virtual Environment Exploration},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {Lost In Space is a project utilizing Bluetooth Low Energy beacons in tandem with mobile devices and 3D panning on the web to overlay virtual sound arrangements onto a physical location in which users can listen through their phones and tablets. The virtual environments are dis- tributed through a website and are populated with virtual sounds and speaker locations. Users activate the Bluetooth on their mobile devices to scan for beacons. User location navigates the corresponding location in virtual environment. Moving around the virtual environment probes the sound- scape. The project also touches on issues of unreliability of Bluetooth tracking indoors, the state of BLE-based project development, and the need for Web Bluetooth development to enable these types of projects.},
file      = {:Users/Hjem/Documents/WAC Papers/2018/lost-in-space.pdf:pdf},
type      = {Paper},
}

@InProceedings{2018_13,
author    = {Larkin, Oliver and Harker, Alex and Kleimola, Jari},
title     = {iPlug 2 : Desktop Plug-in Framework Meets Web Audio Modules},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
pages     = {1--6},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {This paper introduces iPlug 2: a desktop C++ audio plug-in framework that has been extended and reworked in order to support Web Audio Modules, a new format for browser-based audio effects and instruments, using WebAssembly. iPlug 2 provides a complete solution and workflow for the development of cross-platform audio plug-ins and apps. It allows the same concise C++ code to be used to create desktop and web-based versions of a software musical instrument or audio effect, including audio signal processing and user interface elements. This new version of the framework has been updated to increase its flexibility so that alternative drawing APIs, plug-in APIs and platform APIs can be supported easily. We have added support for the distributed models used in recent audio plug-in formats, as well as new graphics capabilities. The codebase has also been substantially modernised. In this paper, we introduce the problems that iPlug 2 aims to address and discuss trends in modern plug-in APIs and existing solutions. We then present iPlug 2 and the work required to refactor a desktop plug-in framework to support the web platform. Several approaches to implementing graphical user interfaces are discussed as well as creating remote editors using web technologies. A real-world example of a WAM compiled with iPlug 2 is hosted at https://virtualcz.io, a new web- based version of a commercially available synthesizer plug-in.},
file      = {:Users/Hjem/Documents/WAC Papers/2018/iplug2-desktop-plug-in-framework-meets-web-audio-modules.pdf:pdf},
type      = {Paper},
}

@InProceedings{2018_12,
author    = {Pauwels, Johan and Xambó, Anna and Roma, Gerard and Barthet, Mathieu and Fazekas, György},
title     = {Exploring Real-time Visualisations to Support Chord Learning with a Large Music Collection},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
number    = {Iv},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {A common problem in music education is finding varied and engaging material that is suitable for practising a specific musical concept or technique. At the same time, a number of large music collections are available under a Creative Commons (CC) licence (e.g. Jamendo, ccMixter), but their potential is largely untapped because of the relative obscurity of their content. In this paper, we present Jam with Jamendo, a web application that allows novice and expert learners of musical instruments to query songs by chord content from a large music collection, and practise the chords present in the retrieved songs by playing along. Its goal is twofold: the learners get a larger variety of practice material, while the artists receive increased exposure. We experimented with two visualisation modes. The first is a linear visualisation based on a moving time axis, the second is a circular visualisation inspired by the chromatic circle. We conducted a small-scale thinking-aloud user study with seven participants based on a hands-on practice with the web app. Through this pilot study, we obtained a qualitative understanding of the potentials and challenges of each visualisation, which will be used to inform the next design iteration of the web app.},
file      = {:Users/Hjem/Documents/WAC Papers/2018/exploring-real-time-visualisations-to-support-chord-learning-with-a-large-music-collection.pdf:pdf},
type      = {Paper},
}

@InProceedings{2018_11,
author    = {Dodds, Thomas},
title     = {dspNode : Real-time remote audio rendering},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {The author has been experimenting with various implementations of real-time cloud based audio rendering, keeping the client side application as an extremely light weight remote controller, receiving the fully rendered audio stream from a cloud based audio rendering engine. The general benefits, drawbacks and conclusions will be discussed with a plausible and functional example application given for the reader's own performance evaluation.},
file      = {:Users/Hjem/Documents/WAC Papers/2018/dspnode-real-time-remote-audio-rendering.pdf:pdf},
type      = {Paper},
}