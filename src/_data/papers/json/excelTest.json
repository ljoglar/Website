[{"Files":"","Paper ID":"5","Paper Title":"Independent music producers making revenue from streaming: Can it form part of their business model?","Abstract":"The music industry, along with other media industries has changed due to the disruptive influence of the Internet. Like any change there is a cycle before acceptance. The music industry’s first response was to try and clamp down and sue digital consumer services such as Napster, before going its own individual ways with bespoke download stores. Eventually, services such as iTunes normalised the market offering a one stop solution for producers and consumers. The new challenge is streaming. Empirical evidence suggests that it has reduced the level of piracy, but it remains difficult to establish a sustainable business model in streaming for niche independent music production companies. This paper takes three case studies and considers the potential direction for a localised, curated and tailored future for the music industry as well as a case study of a new curated streaming service.","Author Names":"Steffan Thomas (Bangor University)*","Author Emails":"s.w.thomas@bangor.ac.uk","Primary Contact Author Email":"s.w.thomas@bangor.ac.uk","Track Name":"Papers and Posters"},{"Files":"","Paper ID":"6","Paper Title":"Play the light of Monet : Interactive web-audio/visual networking system with JavaScript","Abstract":"Web technologies have expeditiously developed by programming developer and web company such as some Web applications for audio processing and synthesizing and node.js server platform by JavaScript. Especially, JavaScript has a possibility of incredible and flexible programming language such as interactive factors of dynamic action on the Web.\nThis project aims to find interactive networking collab- oration system through web technologies with JavaScript in real-time that to control musical expression and various multimedia factors by Web-based API controller at each different place such as mobile SNS message chat through node.js server In this paper, we have tried to find interactive networking real-time system through web-based technologies with JavaScript for artistic expression and various multimedia works.","Author Names":"Euy Shick Hong (MARTE)*; Jun Kim (MARTE)","Author Emails":"antaresax@dongguk.edu; music@dongguk.edu","Primary Contact Author Email":"antaresax@dongguk.edu","Track Name":"Papers and Posters"},{"Files":"","Paper ID":"7","Paper Title":"DrSax.js : Web Audio Library and framework","Abstract":"In last few years, web audio technologies have expeditiously developed by web developer and software company such as some web audio library and frameworks for audio processing and synthesizing by JavaScript. conspicuously, JavaScript has developed a possibility of novel and flexible program- ming language that front-end and server side such as inter- active factors of dynamic action on the Web. This paper aims to create a united sound library and framework system through web technologies with JavaScript for musical instrument and voice through Web Audio API for artistic expression and various musical works.","Author Names":"Euy Shick Hong (MARTE)*; Jun Kim (MARTE)","Author Emails":"antaresax@dongguk.edu; music@dongguk.edu","Primary Contact Author Email":"antaresax@dongguk.edu","Track Name":"Papers and Posters"},{"Files":"","Paper ID":"8","Paper Title":"Webxophone : Web Audio wind instrument","Abstract":"In last few years, a musical application has developed by developer and computer software company such as sound effects, midi controller, musical instrument for music play- ing and collaborate musical works in mobile, tablet PC with iPhone or Android. Especially, by Web technologies, Web audio application with Web Audio API have expeditely studied thought JavaScript that possibility of novel and flexible language.The Webxophone is a web wind instrument designed for mobile with the web and mobile technologies that microphone input (for blowing), multitouch (for finger- ing button), Gyro sensors, sound processing and synthesis in real-time on the web without install. This paper aims to present a web wind instrument application like saxophone through Web Audio API with JavaScript for musical ex pression and various musical performance.","Author Names":"Euy Shick Hong (MARTE)*; Jun Kim (MARTE)","Author Emails":"antaresax@dongguk.edu; music@dongguk.edu","Primary Contact Author Email":"antaresax@dongguk.edu","Track Name":"Papers and Posters"},{"Files":"scientific-audio.pdf (158589 bytes)","Paper ID":"9","Paper Title":"Web Technologies for Scientific Hearing Experiments and Teaching - An Overview","Abstract":"Scientists of audio-related fields need to verify their theories by conducting controlled experiments with human test subjects. The process of developing and conducting such experiments often poses non-trivial challenges to scientists and test subjects. Web technologies promise simple delivery of experiments as interactive websites, possibly even on subjects' own computers. Similar benefits are possible for teaching science. While many tasks in hearing experiments and teaching are well-supported with current web-based tools, support for scientific data structures, signal processing operations and statistical data analysis methods is still incomplete in comparison with entrenched non-web tools. These shortcomings could easily be overcome with a few libraries, however, and would provide a great boon to scientists and educators in many areas of research.","Author Names":"Bastian Bechtold (Jade Hochschule)*; Joerg Bitzer (Jade Hochschule); Stephanus Volke (Jade Hochschule)","Author Emails":"bastian.bechtold@jade-hs.de; joerg.bitzer@jade-hs.de; stephanus.volke@jade-hs.de","Primary Contact Author Email":"bastian.bechtold@jade-hs.de","Track Name":"Papers and Posters"},{"Files":"open-band-web.pdf (2847918 bytes)","Paper ID":"11","Paper Title":"Open band: Audience Creative Participation Using Web  Audio Synthesis","Abstract":"This work investigates a web-based open environment enabling collaborative music experiences. We propose an artifact, Open Band, which enables collective sound dialogues in a web ``agora\", blurring the limits between audience and performers. The systems relies on a multi-user chat system where textual inputs are translated to sounds. We depart from individual music playing experiences in favor of creative participation in networked music making. A previous implementation associated typed letters to pre-composed samples. We present and discuss in this paper a novel instance of our system which operates using Web Audio synthesis.","Author Names":"Ariane Stolfi (Universidade de São Paulo)*; Fabio Goródscy (Universidade de São Paulo); Mathieu Barthet (QMUL); Antônio  Carvalho Junior (Universidade de São Paulo); Fernando Iazzetta (Universidade de São Paulo)","Author Emails":"arianestolfi@gmail.com; fabiogorodscy@gmail.com; m.barthet@qmul.ac.uk; deusanyjunior@gmail.com; iazzetta@usp.br","Primary Contact Author Email":"arianestolfi@gmail.com","Track Name":"Papers and Posters"},{"Files":"Audealize_API-3.pdf (827070 bytes)","Paper ID":"12","Paper Title":"A Web Audio Node for the Fast Creation of Natural Language Interfaces for Audio Production","Abstract":"Audio production involves the use of tools such as rever- berators, compressors, and equalizers to transform raw audio into a state ready for public consumption. These tools are in wide use by both musicians and expert audio engineers for this purpose. The typical interfaces for these tools use low-level signal parameters as controls for the audio effect. These signal parameters often have unintuitive names such as “feedback” or “low-high” that have little meaning to many people. This makes them difficult to use and learn for many people. Such low-level interfaces are also common throughout audio production interfaces using the Web Audio API. Recent work in bridging the semantic gap between verbal descriptions of audio effects (e.g. “underwater”, “warm”, “bright”) and low-level signal parameters has resulted in provably better interfaces for a population of laypeople. In that work, a vocabulary of hundreds of descriptive terms was crowdsourced, along with their mappings to audio effects settings for rever- beration and equalization. In this paper, we present a Web Audio node that lets web developers leverage this vocabulary to easily create web-based audio effects tools that use natural language interfaces. Our Web Audio node and additional documentation can be accessed at https://interactiveaudiolab.github.io/audealize_api","Author Names":"Michael Donovan (Northwestern University)*; Prem Seetharaman (Northwestern University); Bryan Pardo (Northwestern University)","Author Emails":"mbdono56@gmail.com; prem@u.northwestern.edu; pardo@cs.northwestern.edu","Primary Contact Author Email":"mbdono56@gmail.com","Track Name":"Papers and Posters"},{"Files":"wac-2017.pdf (243434 bytes)","Paper ID":"17","Paper Title":"BAT: An open-source, web-based audio events annotation tool","Abstract":"In this paper we present BAT (BMAT Annotation Tool), an open-source, web-based tool for the manual annotation of events in audio recordings developed at BMAT (Barcelona Music and Audio Technologies, www.bmat.com). The main feature of the tool is that it provides an easy way to annotate the salience of simultaneous sound sources. Additionally, it allows to define multiple ontologies to adapt to multiple tasks and offers the possibility to cross-annotate audio data. Moreover, it is easy to install and deploy on servers. We carry out an evaluation where 3 annotators use BAT to annotate a small dataset composed of broadcast media recordings. The results of the experiments show that BAT offers fast annotation mechanisms and a method to assign salience that produces high agreement among annotators.","Author Names":"Blai Melendez-Catalan (Universitat Pompeu Fabra (UPF), Music Technology Group (MTG).)*","Author Emails":"blaimelcat@gmail.com","Primary Contact Author Email":"blaimelcat@gmail.com","Track Name":"Papers and Posters"},{"Files":"Web audio conference rapidmix codecircle paper(1).doc (731853 bytes); Web audio conference rapidmix codecircle paper(2).pdf (485618 bytes)","Paper ID":"18","Paper Title":"Write once run anywhere revisited: machine learning and audio tools in the browser with C++ and Emscripten","Abstract":"A methodology for deploying interactive machine learning and audio tools written in C++ across a wide variety of platforms, including web browsers, is described. The work flow involves development of the code base in C++, making use of all the facilities available to C++ programmers, then transpiling to asm.js bytecode, using Emscripten to allow use of the libraries in web browsers.. Audio capabilities are provided via the C++ Maximilian library that is transpiled and connected to the Web Audio API, via the ScriptProcessorNode. Machine learning is provided via the RapidLib library which implements neural networks and K-NN for regression and classification tasks. An online, browser based IDE is the final part of the system.  It aims to make the toolkit available for education and rapid prototyping purposes, without requiring software other than a web browser. Two example use cases are described: rapid prototyping of novel,  electronic instruments and education. Finally, an evaluation of the performance of the libraries is presented, showing that they perform acceptably well in the web browser, compared to the native counterparts but there is room for improvement here. The system is being used by thousands of students in our on-campus and online courses.","Author Names":"Matthew Yee-King (Goldsmiths, University of London)*; Mick Grierson (Goldsmiths University of London); Michael Zbyszynski (Goldsmiths University of London); Leon Fedden (Goldsmiths University of London)","Author Emails":"m.yee-king@gold.ac.uk; M.Grierson@gold.ac.uk; M.Zbyszynski@gold.ac.uk; leonfedden@gmail.com","Primary Contact Author Email":"m.yee-king@gold.ac.uk","Track Name":"Papers and Posters"},{"Files":"web-sonification-sandbox (3).pdf (422532 bytes)","Paper ID":"24","Paper Title":"Web Sonification Sandbox - an Easy-to-Use Web Application for Sonifying Data and Equations","Abstract":"Auditory and multimodal presentation of data (“auditory\ngraphs”) can allow for discoveries in a data set that are sometimes\nimpossible with visual-only inspection. At the same\ntime, multimodal graphs can make data, and the STEM\nfields that rely on them, more accessible to a much broader\nrange of people, including many with disabilities. There\nhave been a variety of software tools developed to turn data\ninto sound, including the widely-used Sonification Sandbox,\nbut there remains a need for simple, powerful, and more accessible\ntool for the construction and manipulation of multimodal\ngraphs. Web-based audio functionality is now at\nthe point where it can be leveraged to provide just such a\ntool. Thus, we developed a web application, the Web Soni-\nfication Sandbox (or simply the Web Sandbox), that allows\nusers to create and manipulate multimodal graphs that convey\ninformation through both sonification and visualization.\nThe Web Sandbox is designed to be usable by individuals\nwith no technical or musical expertise, which separates it\nfrom existing software. The easy-to-use nature of the Web\nSandbox, combined with its multimodal nature, allow it to\nbe a maximally accessible application by a diverse audience\nof users. Nevertheless, the application is also powerful and\nflexible enough to support advanced users.","Author Names":"Zachary Kondak (Georgia Tech)*","Author Emails":"zachk414@gmail.com","Primary Contact Author Email":"zachk414@gmail.com","Track Name":"Papers and Posters"},{"Files":"loop-aware-audio-recording.pdf (772473 bytes)","Paper ID":"25","Paper Title":"Loop-aware Audio Recording for the Web","Abstract":"Music loops are audio recordings used as basic building blocks in many types of music. The use of pre-recorded loops facilitates engagement into music creation to users regardless of their background in music theory. Using online loop databases also affords simple collaboration and exchange. Hence, music loops are particularly attractive for Web Audio applications.\nHowever, traditional musical audio recording currently relies on complex DAW software. Recording loops usually requires consideration of musical meter and tempo, and withstanding metronome sounds. \n\nIn this paper, we propose \"loop-aware\" audio recording as a use-case for Web Audio technologies. Our approach allows for hands-free, low-stress recording of music loops in web-enabled devices. The system is able to detect repetitions in an incoming audio stream. Based on this information, it segments and ranks the repeated fragments, presenting the list to the user. We provide an example implementation, and evaluate the use of the different MIR libraries available in the Web Audio platform for the proposed task.","Author Names":"Gerard Roma (University of Huddersfield)*; Anna Xambó (Georgia Institute of Technology); Jason Freeman (Georgia Institute of Technology)","Author Emails":"gerard.roma@gmail.com; anna.xambo@gatech.edu; jason.freeman@gatech.edu","Primary Contact Author Email":"gerard.roma@gmail.com","Track Name":"Papers and Posters"},{"Files":"Hi_precision_audio_in_listening_tests.pdf (304111 bytes)","Paper ID":"28","Paper Title":"Hi-precision audio in listening tests - also in the browser?","Abstract":"In listening tests, detailed sound control is sometimes mandatory down to each individual digital sample value and guarantee is needed that they are not unintentionally altered. At other times, a lesser degree of control is acceptable, if on the other hand test execution becomes less restricted. Detailed control of sound is often possible only under \"laboratory\" conditions where hardware and software are under complete control and sound pressure levels can be accurately calibrated. On the other hand, if test persons can do listening tests at home, via an internet browser for example, collecting large amounts of data becomes faster and cheaper (no laboratory facilities required, and more persons can do tests in parallel). Online listening tests made possible by the Web Audio API offers great flexibility in test execution, but compromises in precise stimulus control must be accepted. This paper analyzes such compromises by discussing technological limitations of Web Audio API followed by validation measurements of sound playback in popular internet browsers. The measurements show that at the detailed level there are significant differences in actual performance of different browsers and behavior is not always as expected. Finally, a solution is presented where audio presentation is delegated to an external audio presenter for situations where the limitations of Web Audio API are not acceptable.","Author Names":"Benjamin Pedersen (DELTA)*","Author Emails":"benjaminpedersen@outlook.com","Primary Contact Author Email":"benjaminpedersen@outlook.com","Track Name":"Papers and Posters"},{"Files":"intelligent-audio-workstation (1).pdf (582643 bytes)","Paper ID":"37","Paper Title":"Intelligent audio workstation in the browser","Abstract":"Music production is a complex process requiring skill and time to undertake. The industry has undergone a digital revolution, but unlike other industries the process has not changed. However, intelligent systems, using the semantic web and signal processing, can reduce this complexity by making certain decisions for the user with minimal interaction, saving both time and investment on the engineers' part. This paper will outline an intelligent Digital Audio Workstation (DAW) in designed for use in the browser. It outlines the architecture of the DAW with its audio engine (built on the Web Audio API), using AngularJS for the user interface and a relational database.","Author Names":"Nicholas Jillings (Birmingham City University)*; Ryan Stables (Birmingham City University)","Author Emails":"nicholas.jillings@mail.bcu.ac.uk; ryan.stables@bcu.ac.uk","Primary Contact Author Email":"nicholas.jillings@mail.bcu.ac.uk","Track Name":"Papers and Posters"},{"Files":"2017-wac-lfo_camera-ready.pdf (464939 bytes)","Paper ID":"39","Paper Title":"LFO – A Graph-based Modular Approach to the Processing of Data Streams","Abstract":"This paper introduces \"lfo\" — for Low Frequency Operators — a graph-based Javascript (ES2015) API for online and offline processing (i.e. analysis and transformation) of data streams such as audio and motion sensor data. The library is open-source and entirely based on web standards. The project aims at creating an ecosystem consisting of platform-independent stream operator modules such as filters and extractors as well as platform-specific source and sink modules such as audio i/o, motion sensor inputs, and file access. The modular approach of the API allows for using the library in virtually any Javascript environment. A first set of operators as well as basic source and sink modules for web browsers and Node.js are included in the distribution of the library. The paper introduces the underlying concepts, describes the implementation of the API, and reports on benchmarks of a set of operators. It concludes with the presentation of a set of example applications.","Author Names":"Benjamin Matuszewski (Ircam)*; Norbert Schnell (Ircam)","Author Emails":"benjamin.matuszewski@ircam.fr; Norbert.Schnell@ircam.fr","Primary Contact Author Email":"benjamin.matuszewski@ircam.fr","Track Name":"Papers and Posters"},{"Files":"WasabiFinal.pdf (2029613 bytes)","Paper ID":"40","Paper Title":"The WASABI Project: a 2 million song database with audio and cultural metadata plus WebAudio enhanced client applications","Abstract":"The WASABI project (started in early 2017), is a 42-month project founded by the French National Agency for Research (ANR). Partners in this project are: the I3S laboratory from CNRS, IRCAM, DEEZER, and a private company named Parisson. Other collaborators are Radio France (journalists, archivists), music composers, musicologists, music schools, sound engineering schools.\nIts primary goal consists in building a 2 million song knowledge base that contains metadata collected from music databases on the Web (artists, discography, producers, year of production, etc.), from the analysis of song lyrics (what are they talking about? are locations or people mentioned? Which emotions do they convey? What is the structure of the song lyrics?), and from the audio analysis (beat, loudness, chords, structure, cover detection, source separation / demixing).\nThe main originality of this project is the collaboration between the algorithms that will extract semantic metadata from the web and from song lyrics with the algorithms that will work on the audio. Unmixing (separate the different audio sources from a stereo audio file) can be enhanced if we know in advance the orchestration, genre detection can be more accurate if we know the song topic and emotions from the words used in its lyrics, etc.\nA preliminary version of this database is already online and will be enriched during the duration of this project. WebAudio enhanced applications will be associated with each song in this database. An online mixing table, guitar amp simulations with a virtual pedalboard, audio analysis visualization tools, annotation tools, a similarity search tool that works by uploading audio extracts or playing some melody using a MIDI device, etc. are planned as companions for the WASABI database. Some prototype tools have already been published or demoed at different conferences.","Author Names":"Michel Buffa (Université Côte d’Azur, CNRS, INRIA)*; Catherine Faron-Zucker (Université Côte d’Azur, CNRS, INRIA); Alain Giboin (Université Côte d’Azur, CNRS, INRIA); Isabelle Mirbel (Université Côte d’Azur, CNRS, INRIA); Elena Cabrio (Université Côte d’Azur, CNRS, INRIA); Geoffroy Peeters (Telecom ParisTech); Guillaume Pelerin (IRCAM); Thomas Fillon (PARISSON); Manuel Moussalam (DEEZER); Romain Hennequin (DEEZER); Gabriel Meseguer Brocal (IRCAM); Francesco Piccoli (DEEZER); Fabrice Jauvat (Université Côte d’Azur, CNRS, INRIA); Elmahdi Korfed (Université Côte d’Azur, CNRS, INRIA)","Author Emails":"buffa@i3s.unice.fr; faron@i3s.unice.fr; alain.giboin@inria.fr; isabelle.mirbel@i3s.unice.fr; Elena.cabrio@i3s.unice.fr; geoffroy.peeters@telecom-paristech.fr; Guillaume.Pellerin@ircam.fr; thomas@parisson.com; manuel.moussallam@deezer.com; rhennequin@deezer.com; Gabriel.MeseguerBrocal@ircam.fr; fpiccoli@deezer.com; fabrice.jauvat@i3s.unice.fr; elmahdi.korfed@i3s.unice.fr","Primary Contact Author Email":"buffa@i3s.unice.fr","Track Name":"Papers and Posters"},{"Files":"","Paper ID":"48","Paper Title":"Sound recycling from public databases","Abstract":"Discovering new sounds from online databases is a tedious task. Standard search tools and manual exploration fails to manage the actual amount of information available.This paper presents a new approach to the problem which takes advantage of grown technologies like Big Data and Machine Learning,  keeping in mind compositional concepts and focusing on artistic performances. Among several different distributed systems useful for music experimentation, a new workflow is proposed based on analysis techniques from Music Information Retrieval (MIR) combined with massive online databases, dynamic user interfaces, physical controllers and real-time synthesis. Based on Free Software tools and standard communication protocols to classify, cluster and segment sound. The control architecture allows multiple clients request the API services concurrently enabling collaborative work. The resulting system can retrieve well defined or pseudo-aleatory, audio samples from the web, mix and transform them in real-time during a live-coding performance, play like another instrument in a band, as a solo artist combined with visual feedback or working alone as automated multimedia installation.","Author Names":"Hernán Ordiales (RedPanal)*; Matias Lennie Bruno (Redpanal.org)","Author Emails":"hordiales@gmail.com; matias.lennie@gmail.com","Primary Contact Author Email":"hordiales@gmail.com","Track Name":"Papers and Posters"},{"Files":"paper-17-07-18.pdf (953387 bytes)","Paper ID":"51","Paper Title":"trackswitch.js: A Versatile Web-Based Audio Player for Presenting Scientific Results","Abstract":"trackswitch.js is a versatile web-based audio player that enables researchers to conveniently present examples and results from scientific audio processing applications. Based on a multitrack architecture, trackswitch.js allows a listener to seamlessly switch between multiple audio tracks, while synchronously indicating the playback position within images associated to the audio tracks. These images may correspond to feature representations such as spectrograms or to visualizations of annotations such as structural boundaries or musical note information. The provided switching and playback functionalities are simple yet useful tools for analyzing, navigating, understanding, and evaluating results obtained from audio processing algorithms. Furthermore, trackswitch.js is an easily extendible and manageable software tool, designed for non-expert developers and unexperienced users. Offering a small but useful selection of options and buttons, trackswitch.js requires only basic knowledge to implement a versatile range of components for web-based audio demonstrators and user interfaces. Besides introducing the underlying techniques and the main functionalities of trackswitch.js we provide several use cases that indicate the flexibility and usability of our software for different audio-related research areas.","Author Names":"Nils Werner (International Audio Laboratories Erlangen)*; Stefan Balke (International Audio Laboratories Erlangen); Fabian-Robert Stöter (International Audio Laboratories Erlangen); Meinard Müller (International Audio Laboratories Erlangen); Bernd Edler (International Audio Laboratories Erlangen)","Author Emails":"nils.werner@audiolabs-erlangen.de; stefan.balke@audiolabs-erlangen.de; fabian-robert.stoeter@audiolabs-erlangen.de; meinard.mueller@audiolabs-erlangen.de; bernd.edler@audiolabs-erlangen.de","Primary Contact Author Email":"nils.werner@audiolabs-erlangen.de","Track Name":"Papers and Posters"},{"Files":"PracticalSoundmapping.Final.pdf (407949 bytes)","Paper ID":"59","Paper Title":"Practicable Soundmapping:  JavaScript enabled Edge Compute","Abstract":"In this paper, we report on developments of the Citygram project, which provides a comprehensive platform to capture, stream, analyze, visualize, and provide easy access to spatiotemporal soundscape data. Launched in 2011, Citygram’s recent strategic decision has resulted in system migration from compiled languages to JavaScript. Citygram now runs on standard web-browsers or in node.js significantly alleviating problems concerning core soundmapping system complexities including operating system limitations, hardware dependency, software update and dissemination issues, data access mechanism, data visualization, and cost. This strategy has made practicable a key design philosophy for soundmapping: rapid sensor network growth for spatiotemporally granular soundscape capture through community and citizen-scientist engagement. We summarize research and development for the following modules (1) sensor module allowing high-value data transmission through edge compute paradigms, (2) machine learning module focusing on environmental sound classification, (3) visualization and data access prototypes.","Author Names":"Tae Hong Park (NYU)*; Minjoon Yoo (NYU)","Author Emails":"thp1@nyu.edu; minjoon.yoo@nyu.edu","Primary Contact Author Email":"thp1@nyu.edu","Track Name":"Papers and Posters"},{"Files":"Faust-web-wac.pdf (931176 bytes)","Paper ID":"60","Paper Title":"Compiling Faust audio DSP code to WebAssembly","Abstract":"After a first version based on asm.js [4], we show in this paper how the Faust audio DSP language can be used to generate efficient Web Audio nodes based on WebAssem- bly. Two new compiler backends have been developed. The libfaust library version of the compiler has been compiled for the Web, thus allowing to have an efficient compilation chain from Faust DSP sources and libraries to audio nodes directly available in the browser.","Author Names":"Stéphane  Letz (GRAME)*; Yann Orlarey (GRAME); Dominique Fober (GRAME)","Author Emails":"letz@free.fr; orlarey@grame.fr; fober@grame.fr","Primary Contact Author Email":"letz@free.fr","Track Name":"Papers and Posters"},{"Files":"framework-discovery-collections.pdf (2636963 bytes)","Paper ID":"61","Paper Title":"Towards a Framework for the Discovery of Collections of Live Music Recordings and Artefacts on the Semantic Web","Abstract":"This paper introduces a platform for the representation and discovery of live music recordings and associated artefacts based on a dedicated data model. We demonstrate our technology by implementing a Web-based discovery tool for the Grateful Dead collection of the Internet Archive, a large collection of concert recordings annotated with editorial metadata. We represent this information using a Linked Data model complemented with data aggregated from several additional Web resources discussing and describing these events. These data include descriptions and images of physical artefacts such as tickets, posters and fan photos, as well as other information, e.g. about location and weather. The system uses signal processing techniques for the analysis and alignment of the digital recordings. During the discovery, users can juxtapose and compare different recordings of a given concert, or different performances of a given song by interactively blending between them.","Author Names":"Thomas Wilmering (Queen Mary University of London)*; Florian Thalmann (Queen Mary University of London); Mark B. Sandler (Queen Mary University of London)","Author Emails":"t.wilmering@qmul.ac.uk; f.thalmann@qmul.ac.uk; mark.sandler@qmul.ac.uk","Primary Contact Author Email":"t.wilmering@qmul.ac.uk","Track Name":"Papers and Posters"},{"Files":"exploring-musical-expression-final.pdf (336885 bytes)","Paper ID":"62","Paper Title":"Exploring Musical Expression on the Web by Deforming, Exaggerating, and Blending Audio Recordings of Performances","Abstract":"We introduce a prototype of an educational web application for comparative performance analysis based on source separation and object-based audio techniques. The underlying system decomposes recordings of classical music performances into note events using score-informed source separation and represents the decomposed material using semantic web technologies. In a visual and interactive way, users can explore individual performances by highlighting specific musical aspects directly within the audio and by altering the temporal characteristics to obtain versions in which the micro-timing is exaggerated or suppressed. Multiple performances of the same work can be compared by juxtaposing and blending between the corresponding recordings. Finally, by adjusting the timing of events, users can generate intermediates of multiple performances to investigate their commonalities and differences.","Author Names":"Florian Thalmann (Queen Mary University of London); Sebastian Ewert (Queen Mary University of London)*; Geraint Wiggins (Vrije Universiteit Brussel); Mark B. Sandler (Queen Mary University of London)","Author Emails":"f.thalmann@qmul.ac.uk; s.ewert@qmul.ac.uk; geraint.wiggins@qmul.ac.uk; mark.sandler@qmul.ac.uk","Primary Contact Author Email":"s.ewert@qmul.ac.uk","Track Name":"Papers and Posters"},{"Files":"WAC_2017_SoundworksNu_v2.1.pdf (1214707 bytes)","Paper ID":"63","Paper Title":"Nü Soundworks: Using spectators smartphones as a distributed network of speakers and sensors during live performances.","Abstract":"This paper presents the Nü framework. The objective of the framework is to provide composers with a tool to control web-based audio processes on spectators smartphones during live performances. Connecting their devices to a webpage broadcasted by the performer's laptop, spectators become part of the composition: from simple sound sources to active musicians. From a Max based interface, the performer can then control the behaviours of conceptual units, referred to as Nü modules, designed for live composition (distributed room reverb, granular synthesis, etc.). Each module is composed of a pair of JavaScript classes - one for the client, another for the server - relying on the Web Audio API for audio processing, and OSC messages for parameters control. Nü is an open source project based on the Soundworks framework.","Author Names":"David Poirier-Quinot (IRCAM)*; Norbert Schnell (Ircam); Olivier Warusfel (IRCAM)","Author Emails":"davipoir@gmail.com; Norbert.Schnell@ircam.fr; olivier.warusfel@ircam.fr","Primary Contact Author Email":"davipoir@gmail.com","Track Name":"Papers and Posters"},{"Files":"WAC2017 - CORONAL.pdf (446045 bytes); WAC2017 - POSTER.pdf (122436 bytes)","Paper ID":"67","Paper Title":"ARCADE 3D-audio codec: an implementation for the web","Abstract":"This poster introduces the implementation of the ARCADE 3D-audio codec for web browsers.\nARCADE can embed a full 3D audio scene in a simple stereo-compatible audio stream that can be further compressed with standard lossy compression schemes, aired to analog or digital radio receivers or even stored on analog supports. An ARCADE-encoded stream can be decoded to any 2D or 3D-audio rendering format, for instance using Vector-Based Amplitude Panning (VBAP), Higher Order Ambisonics (HOA), or personalized binaural with headtracking.\nARCADE adapts seamlessly to the audio industry needs, from storage to production, distribution/delivery, and rendering. It finds uses in Virtual or Augmented Reality (VR/AR), movies, gaming, music, telepresence & teleconferencing.\nWe present a JavaScript (JS) and Web Audio API implementation of the ARCADE decoder, which was originally written in C++11, along with technical details of the porting operations. Live demos of 3D-audio content transmission, decoding and dynamic binaural rendering will be given during the poster session.","Author Names":"François Becker (Coronal Audio)*; Benjamin Bernard (Coronal Audio); Clément Carron (Coronal Audio)","Author Emails":"francois@coronal.audio; benjamin@coronal.audio; clement@coronal.audio","Primary Contact Author Email":"francois@coronal.audio","Track Name":"Papers and Posters"},{"Files":"","Paper ID":"68","Paper Title":"Usage of Physics Engines for UI Design in NexusUI","Abstract":"In preparation to expand NexusUI, the authors have decided to utilize physics engines for new NexusUI widgets. This research is exploring new avenues of physics-based user interfaces on the web. Tying physics-based user interfaces to web audio encourages exploration of non-linear connections to sound synthesis. The research is meant to study the benefits of liquid simulation alongside rigid bodies when used in sound synthesis and aims to answer whether or not physics-based user interfaces enhance performances. The two physics engines being utilized for the new NexusUI collections are LiquidFun and Matter.js. The new NexusUI widget prototypes are going to take advantage of LiquidFun's Elastic Particles and Matter.js' Cloth to compare uniform distribution disruption with the intent of investigating what 2D space means to music. One of our goals is to find methods of audio synthesis that complement the behaviors of each physics engine. The research breaks down to:\nWhy physics engines?\n\tWhat new avenues open for web audio?\n\tAre interactions within physics engines more intuitive than those \t\twithout?\n\tDo they encourage more exploration than those without?\n\tWhat about non-linear connections to musical parameters?\n\tDo svg’s help us utilize physics engines?\nWhat are the benefits of each physics engine and what can they bring to web audio?\n\tLiquid simulation vs rigid bodies\nHow do we take advantage of elements of each library? (one tackles surface tension, etc.)\n\tSection dedicated to comparing and contrasting the two libraries\nHow can they enhance performance?\n\nUsage of Physics Engines with new NexusUI widgets for new user interfaces is a part of expanding the NexusUI library with new widget collections. NexusUI aims to create new interfaces for audio performance in the browser by taking advantage of unique properties of certain engines. The NexusUI team is researching the pros cons of the applications of each physics engine to develop widgets that play to their strengths.","Author Names":"Chase Mitchusson (LSU)*","Author Emails":"cmtchssn@gmail.com","Primary Contact Author Email":"cmtchssn@gmail.com","Track Name":"Papers and Posters"},{"Files":"wac.2017.pdf (677405 bytes)","Paper ID":"72","Paper Title":"Strategies for Per-Sample Processing of Audio Graphs in the Browser","Abstract":"Due to current browser limitations, most synthesis in the browser is currently performed using the block-rate nodes included in the WebAudio API. However, block-rate processing of audio graphs precludes many types of synthesis in addition to limiting both the accuracy and flexibility of scheduling. We describe alternative strategies for performing efficient, per-sample processing of audio graphs in the browser using the ScriptProcessor node, affording synthesis techniques that are not commonly found in existing JavaScript audio libraries. We introduce a new library, Genish.js, that provides unit generators for common low-level synthesis tasks and acts as a compiler for signal processing functions; this library is a loose port of the Gen framework for Max/MSP. We used Genish.js to update a higher-level library for audio programming, Gibberish.js, realizing improvements to both efficiency and audio quality. Preliminary benchmarks comparing the performance of Genish.js audio graphs to equivalent graphs made with the WebAudio API show promising results.","Author Names":"Charlie Roberts (Worcester Polytechnic Institute)*","Author Emails":"charlie@charlie-roberts.com","Primary Contact Author Email":"charlie@charlie-roberts.com","Track Name":"Papers and Posters"},{"Files":"wac-2017-active-final.pdf (369889 bytes)","Paper ID":"80","Paper Title":"Active Server Roles for Extended Distributed Performance Complexity in Diamonds in Dystopia","Abstract":"Distributed performance systems that utilize a centralized server for connectivity have the potential to also provide extended computational and storage resources that would not be beneficial or even possible if distributed onto mobile clients. The usage of large datasets, shared or collaborative resources, and processor intensive techniques can be performed on the server while allowing time sensitive and less complex user specific computation to occur on client devices. Many of these types of problems can be solved using tools developed for cloud computing. This approach is demonstrated in the work \"Diamonds in Dystopia\", a collaborative poetry performance that incorporates audience interaction on mobile devices, generation of poetic material on server side resources, real-time synthesis distributed through mobile and venue speakers, a live poetry reading performance and the live synthesis of poetry from the collective ensemble.","Author Names":"Jesse Allison (Louisiana State University)*; Derick Ostrenko (Louisiana State University); Vincent Cellucci (Louisiana State University)","Author Emails":"jtallison@lsu.edu; dostrenko@lsu.edu; vcellu1@lsu.edu","Primary Contact Author Email":"jtallison@lsu.edu","Track Name":"Papers and Posters"},{"Files":"Cloudspeakers - a mobile performance network.pdf (345254 bytes)","Paper ID":"83","Paper Title":"Cloudspeakers - a mobile performance network","Abstract":"In this project we developed a network of cloudspeakers. These are mobile speakers connected to a raspberry pi3 equipped with a lowlatency audio card. They are connected to a wifi network and run a SuperCollider-Server (scsynth). Our cloudspeaker can be addressed with the SuperCollider (sclang) in the network. For this purpose we had to create a stable and scaleable network, and we had to find\nsolutions for problems like latency, jitter, or software management. This network can be used for artistic projects and can be combined with a webserver and the use of mobile devices.","Author Names":"Raimund Vogtenhuber (University of Arts Zurich / ICST)*","Author Emails":"raimund.vogtenhuber@zhdk.ch","Primary Contact Author Email":"raimund.vogtenhuber@zhdk.ch","Track Name":"Papers and Posters"},{"Files":"wac-84-final.pdf (1843771 bytes)","Paper ID":"84","Paper Title":"Synchronized mobile devices using web audio technology on a Raspberry Pi","Abstract":"This paper describes the ongoing development of a system\nfor the creation of a distributed musical space: the MusicBox.\nThe MusicBox has been realized as an open access\npoint for mobile devices. It provides a musical web application\nenabling the musician to distribute audio events\nonto the connected mobile devices and control synchronous\nplayback of these events.","Author Names":"Jan-Torsten Milde (Fulda university of applied science)*","Author Emails":"milde@hs-fulda.de","Primary Contact Author Email":"milde@hs-fulda.de","Track Name":"Papers and Posters"}]